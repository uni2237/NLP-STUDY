{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test_Site = http://example.webscraping.com/places/default/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "header = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDownload(url, param = None, retries = 3):\n",
    "    resp = None\n",
    "    try:\n",
    "        resp = requests.get(url, params = param, headers = header)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if 500 <= resp.status_code < 600 and retries > 0:\n",
    "            print('Retries : {0}'.format(retries))\n",
    "            return getDownload(url, param, retries -1)\n",
    "        else:\n",
    "            print(resp.status_code)\n",
    "            print(resp.reason)\n",
    "            print(resp.request.headers)\n",
    "            \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], ['http://example.webscraping.com/places/default/index'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLs = list()\n",
    "visitedURLs = list()\n",
    "\n",
    "URLs.append('http://example.webscraping.com/places/default/index')\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(0)\n",
    "    \n",
    "    resp = getDownload(url)\n",
    "    dom = BeautifulSoup(resp.text, 'lxml')\n",
    "    \n",
    "#    print(len(dom.find_all('a')))\n",
    "    links = [_ for _ in dom.select('a') if _.has_attr('href')]\n",
    "    \n",
    "    visitedURLs.append(url)\n",
    "    \n",
    "URLs, visitedURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/places/default/index', '/places/default/user/register')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [requests.compat.urljoin(url, _['href']) for _ in links]\n",
    "requests.utils.urlparse(temp[0])[2],\\\n",
    "requests.utils.urlparse(temp[1])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://example.webscraping.com/places/default/user/register?_next=/places/default/index',\n",
       " 'http://example.webscraping.com/places/default/user/login?_next=/places/default/index',\n",
       " 'http://example.webscraping.com/places/default/index',\n",
       " 'http://example.webscraping.com/places/default/search',\n",
       " 'http://example.webscraping.com/places/default/view/Afghanistan-1',\n",
       " 'http://example.webscraping.com/places/default/view/Aland-Islands-2',\n",
       " 'http://example.webscraping.com/places/default/view/Albania-3',\n",
       " 'http://example.webscraping.com/places/default/view/Algeria-4',\n",
       " 'http://example.webscraping.com/places/default/view/American-Samoa-5',\n",
       " 'http://example.webscraping.com/places/default/view/Andorra-6',\n",
       " 'http://example.webscraping.com/places/default/view/Angola-7',\n",
       " 'http://example.webscraping.com/places/default/view/Anguilla-8',\n",
       " 'http://example.webscraping.com/places/default/view/Antarctica-9',\n",
       " 'http://example.webscraping.com/places/default/view/Antigua-and-Barbuda-10',\n",
       " 'http://example.webscraping.com/places/default/index/1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [requests.compat.urljoin(url, _['href']) for _ in links if requests.utils.urlparse(_['href'])[2]]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://example.webscraping.com/places/default/user/register?_next=/places/default/index', 'http://example.webscraping.com/places/default/user/login?_next=/places/default/index', 'http://example.webscraping.com/places/default/index', 'http://example.webscraping.com/places/default/search', 'http://example.webscraping.com/places/default/view/Afghanistan-1', 'http://example.webscraping.com/places/default/view/Aland-Islands-2', 'http://example.webscraping.com/places/default/view/Albania-3', 'http://example.webscraping.com/places/default/view/Algeria-4', 'http://example.webscraping.com/places/default/view/American-Samoa-5', 'http://example.webscraping.com/places/default/view/Andorra-6', 'http://example.webscraping.com/places/default/view/Angola-7', 'http://example.webscraping.com/places/default/view/Anguilla-8', 'http://example.webscraping.com/places/default/view/Antarctica-9', 'http://example.webscraping.com/places/default/view/Antigua-and-Barbuda-10', 'http://example.webscraping.com/places/default/index/1']\n"
     ]
    }
   ],
   "source": [
    "URLs = list()\n",
    "visitedURLs = list()\n",
    "\n",
    "URLs.append('http://example.webscraping.com/places/default/index')\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(0)\n",
    "    \n",
    "    resp = getDownload(url)\n",
    "    dom = BeautifulSoup(resp.text, 'lxml')\n",
    "  \n",
    "    links = [requests.compat.urljoin(url, _['href']) \n",
    "             for _ in dom.select('a') \n",
    "             if _.has_attr('href') and \n",
    "             requests.utils.urlparse(_['href']) [2]]   \n",
    "    \n",
    "    print([_ for _ in links])\n",
    "    \n",
    "    visitedURLs.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLs = list()\n",
    "visitedURLs = list()\n",
    "\n",
    "URLs.append('http://example.webscraping.com/places/default/index')\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(0)\n",
    "    visitedURLs.append(url)\n",
    "    \n",
    "    resp = getDownload(url)\n",
    "    dom = BeautifulSoup(resp.text, 'lxml')\n",
    "  \n",
    "    links = [requests.compat.urljoin(url, _['href']) \n",
    "             for _ in dom.select('a') \n",
    "             if _.has_attr('href') and \n",
    "             requests.utils.urlparse(_['href']) [2]]  \n",
    "    \n",
    "    for link in links:\n",
    "        if link not in URLs and link not in visitedURLs:\n",
    "            URLs.append(link)\n",
    "    break\n",
    "    \n",
    "len(URLs), len(visitedURLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1\n",
      "15 2\n",
      "14 3\n",
      "15 4\n",
      "24 5\n"
     ]
    }
   ],
   "source": [
    "import time, random\n",
    "\n",
    "URLs = list()\n",
    "lastURL = ''\n",
    "visitedURLs = list()\n",
    "\n",
    "URLs.append('http://example.webscraping.com/places/default/index')\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(0)\n",
    "    visitedURLs.append(url)\n",
    "    \n",
    "    resp = getDownload(url)\n",
    "    \n",
    "    if not resp:\n",
    "        URLs.append(url)\n",
    "        continue\n",
    "        \n",
    "    dom = BeautifulSoup(resp.text, 'lxml')\n",
    "  \n",
    "    links = [requests.compat.urljoin(url, _['href']) \n",
    "             for _ in dom.select('a') \n",
    "             if _.has_attr('href') and \n",
    "             requests.utils.urlparse(_['href'])[2]]  \n",
    "    \n",
    "    for link in links:\n",
    "        if link not in URLs and link not in visitedURLs:\n",
    "            URLs.append(link)\n",
    "            \n",
    "    print(len(URLs), len(visitedURLs))\n",
    "    \n",
    "    if requests.utils.urlparse(url)[1] == lastURL:\n",
    "        time.sleep(random.randint(5,10))\n",
    "        \n",
    "    \n",
    "    lastURL= requests.utils.urlparse(url)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test_Site = http://pythonscraping.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focused Crawling\n",
    "1. Domain \n",
    "2. Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = list()\n",
    "visitedURLs = list()\n",
    "\n",
    "URLs.append('http://pythonscraping.com/')\n",
    "\n",
    "lastURL = ''\n",
    "baseURL = requests.utils.urlparse(URLs[0])[1]\n",
    "\n",
    "while URLs:\n",
    "    url = URLs.pop(0)\n",
    "    visitedURLs.append(url)\n",
    "    \n",
    "    resp = getDownload(url)\n",
    "    \n",
    "    if not resp:\n",
    "#        URLs.append(url)\n",
    "        continue\n",
    "        \n",
    "    dom = BeautifulSoup(resp.text, 'lxml')\n",
    "  \n",
    "    links = [requests.compat.urljoin(url, _['href']) \n",
    "             for _ in dom.select('a') \n",
    "             if _.has_attr('href') and \n",
    "             requests.utils.urlparse(_['href'])[2]]\n",
    "    \n",
    "    for link in links:\n",
    "        if baseURL == requests.utils.urlparse(link)[1]:\n",
    "            if link not in URLs and link not in visitedURLs:\n",
    "                URLs.append(link)\n",
    "            \n",
    "    print(len(URLs), len(visitedURLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitedURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = list()\n",
    "visitedURLs = list()\n",
    "\n",
    "URLs.append(('http://pythonscraping.com/',1))\n",
    "\n",
    "lastURL = ''\n",
    "baseURL = requests.utils.urlparse(URLs[0][0])[1]\n",
    "\n",
    "useBase = False\n",
    "useDepth = 3\n",
    "\n",
    "while URLs:\n",
    "    urls = URLs.pop(0)\n",
    "    \n",
    "    url = urls[0]\n",
    "    depth = urls[1] + 1\n",
    "    \n",
    "    visitedURLs.append(urls)\n",
    "    \n",
    "    resp = getDownload(url)\n",
    "    \n",
    "    if not resp:\n",
    "#        URLs.append(url)\n",
    "        continue\n",
    "        \n",
    "    dom = BeautifulSoup(resp.text, 'lxml')\n",
    "  \n",
    "    links = [requests.compat.urljoin(url, _['href']) \n",
    "             for _ in dom.select('a') \n",
    "             if _.has_attr('href') and \n",
    "             requests.utils.urlparse(_['href'])[2]]\n",
    "    \n",
    "    for link in links:\n",
    "        if useBase and baseURL == requests.utils.urlparse(link)[1]:\n",
    "            if link not in [_[0] for _ in URLs] and link not in [_[0] for _ in visitedURLs]:\n",
    "                URLs.append((link, depth))              \n",
    "        if depth < useDepth:\n",
    "            if link not in [_[0] for _ in URLs] and link not in [_[0] for _ in visitedURLs]:\n",
    "                URLs.append((link, depth))\n",
    "            \n",
    "    print(len(URLs), len(visitedURLs))\n",
    "                            \n",
    "    if requests.utils.urlparse(url)[1] == lastURL:\n",
    "        time.sleep(random.randint(5,10))\n",
    "        \n",
    "    lastURL= requests.utils.urlparse(url)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitedURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
